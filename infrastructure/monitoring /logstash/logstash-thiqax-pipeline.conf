# ====================== ThiQaX Main Logstash Pipeline ======================
# This pipeline configuration defines the main log processing for the ThiQaX platform

input {
  # Filebeat input for structured logs (JSON format)
  beats {
    port => 5044
    host => "0.0.0.0"
    client_inactivity_timeout => 300
    add_field => { 
      "[log][source]" => "filebeat"
      "[metadata][collector]" => "filebeat"
    }
  }

  # TCP input for syslog format logs
  tcp {
    port => 5000
    host => "0.0.0.0"
    codec => "json"
    add_field => {
      "[log][source]" => "tcp"
      "[metadata][collector]" => "direct"
    }
  }

  # HTTP input for direct log shipping from applications
  http {
    port => 8080
    host => "0.0.0.0"
    codec => "json"
    add_field => {
      "[log][source]" => "http"
      "[metadata][collector]" => "direct"
    }
  }
}

filter {
  # Add processing timestamp
  mutate {
    add_field => { "[@metadata][processing_timestamp]" => "%{@timestamp}" }
  }

  # Common timestamp parsing
  date {
    match => [ "timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZ", "yyyy-MM-dd HH:mm:ss,SSS" ]
    target => "@timestamp"
    remove_field => [ "timestamp" ]
    tag_on_failure => ["_dateparsefailure"]
  }

  # Add environment information
  mutate {
    add_field => {
      "[metadata][environment]" => "${LS_ENVIRONMENT:staging}"
      "[metadata][platform]" => "thiqax"
    }
  }

  # ThiQaX API specific log processing
  if [service] == "thiqax-api" {
    # Parse ThiQaX API logs
    if [message] {
      json {
        source => "message"
        target => "parsed_json"
        skip_on_invalid_json => true
        tag_on_failure => ["_jsonparsefailure"]
      }

      # Extract fields from parsed JSON
      if "_jsonparsefailure" not in [tags] {
        mutate {
          rename => {
            "[parsed_json][level]" => "[log][level]"
            "[parsed_json][message]" => "[log][message]"
            "[parsed_json][timestamp]" => "[log][original_timestamp]"
            "[parsed_json][requestId]" => "[transaction][id]"
            "[parsed_json][userId]" => "[user][id]"
            "[parsed_json][method]" => "[http][request][method]"
            "[parsed_json][path]" => "[url][path]"
            "[parsed_json][statusCode]" => "[http][response][status_code]"
            "[parsed_json][responseTime]" => "[event][duration]"
          }
          convert => {
            "[event][duration]" => "integer"
            "[http][response][status_code]" => "integer"
          }
          remove_field => [ "parsed_json", "message" ]
        }
      }
    }

    # Enhance log entries with request categorization
    if [url][path] {
      grok {
        match => { "[url][path]" => "/api/v1/%{WORD:api.endpoint}/%{NOTSPACE:api.resource}/?(?:%{NOTSPACE:api.id})?" }
        tag_on_failure => ["_apiendpointparsefailure"]
        remove_field => [ "message" ]
      }
    }

    # Categorize response status
    if [http][response][status_code] {
      if [http][response][status_code] >= 200 and [http][response][status_code] < 300 {
        mutate { add_field => { "[http][response][category]" => "success" } }
      } else if [http][response][status_code] >= 300 and [http][response][status_code] < 400 {
        mutate { add_field => { "[http][response][category]" => "redirect" } }
      } else if [http][response][status_code] >= 400 and [http][response][status_code] < 500 {
        mutate { add_field => { "[http][response][category]" => "client_error" } }
      } else if [http][response][status_code] >= 500 {
        mutate { add_field => { "[http][response][category]" => "server_error" } }
      }
    }

    # Add event type and category
    mutate {
      add_field => {
        "[event][dataset]" => "thiqax.api"
        "[event][category]" => "web"
      }
    }
  }

  # Parse Nginx access logs
  else if [fileset][name] == "access" or [service] == "nginx" {
    if [message] =~ /^{.*}$/ {
      json {
        source => "message"
        target => "parsed_json"
        skip_on_invalid_json => true
        tag_on_failure => ["_jsonparsefailure"]
      }

      if "_jsonparsefailure" not in [tags] {
        mutate {
          rename => {
            "[parsed_json][remote_addr]" => "[client][ip]"
            "[parsed_json][remote_user]" => "[user][name]"
            "[parsed_json][request]" => "[http][request][original]"
            "[parsed_json][status]" => "[http][response][status_code]"
            "[parsed_json][body_bytes_sent]" => "[http][response][body][bytes]"
            "[parsed_json][http_referer]" => "[http][request][referrer]"
            "[parsed_json][http_user_agent]" => "[user_agent][original]"
            "[parsed_json][http_x_forwarded_for]" => "[client][forwarded_ip]"
            "[parsed_json][request_time]" => "[event][duration]"
          }
          convert => {
            "[http][response][status_code]" => "integer"
            "[http][response][body][bytes]" => "integer"
            "[event][duration]" => "float"
          }
          remove_field => [ "parsed_json", "message" ]
        }
      }
    } else {
      grok {
        match => { "message" => "%{IPORHOST:client.ip} - %{USER:user.name} \[%{HTTPDATE:timestamp}\] \"%{WORD:http.request.method} %{NOTSPACE:url.original} HTTP/%{NUMBER:http.version}\" %{NUMBER:http.response.status_code} %{NUMBER:http.response.body.bytes} \"%{DATA:http.request.referrer}\" \"%{DATA:user_agent.original}\"( \"%{NOTSPACE:client.forwarded_ip}\")?( %{NUMBER:event.duration:float})?" }
        remove_field => [ "message" ]
      }
    }

    # Extract URL parts
    if [url.original] or [http][request][original] {
      grok {
        match => { 
          "[url][original]" => "%{NOTSPACE:url.path}(?:\?%{NOTSPACE:url.query})?"
        }
        match => { 
          "[http][request][original]" => "%{WORD:http.request.method} %{NOTSPACE:url.path}(?:\?%{NOTSPACE:url.query})? HTTP/%{NUMBER:http.version}"
        }
        tag_on_failure => ["_urlparsefailure"]
      }
    }

    # Parse user agent
    if [user_agent][original] {
      useragent {
        source => "[user_agent][original]"
        target => "user_agent"
      }
    }

    # Add event type and category
    mutate {
      add_field => {
        "[event][dataset]" => "nginx.access"
        "[event][category]" => "web"
      }
    }
  }

  # Parse system logs
  else if [service] == "system" {
    # Syslog pattern parsing
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:system.syslog.timestamp} %{SYSLOGHOST:host.hostname} %{DATA:process.name}(?:\[%{POSINT:process.pid}\])?: %{GREEDYDATA:message}" }
      overwrite => [ "message" ]
    }

    # Add event type and category
    mutate {
      add_field => {
        "[event][dataset]" => "system.syslog"
        "[event][category]" => "system"
      }
    }
  }

  # General cleanup and normalization
  # Convert log levels to lowercase
  if [log][level] {
    mutate {
      lowercase => [ "[log][level]" ]
    }
  }

  # Normalize log levels
  if [log][level] in ["warning", "warn"] {
    mutate {
      replace => { "[log][level]" => "warn" }
    }
  }
  if [log][level] in ["err", "error", "critical", "crit", "alert", "emerg"] {
    mutate {
      replace => { "[log][level]" => "error" }
    }
  }
  if [log][level] in ["info", "information", "notice"] {
    mutate {
      replace => { "[log][level]" => "info" }
    }
  }
  if [log][level] in ["debug", "fine", "trace", "verbose"] {
    mutate {
      replace => { "[log][level]" => "debug" }
    }
  }

  # Add hostname and processing metadata
  ruby {
    code => "
      event.set('[host][name]', Socket.gethostname) unless event.get('[host][name]')
      event.set('[metadata][processing][timestamp]', Time.now.strftime('%Y-%m-%dT%H:%M:%S.%LZ'))
    "
  }

  # Add fingerprint for event deduplication
  fingerprint {
    source => ["host", "@timestamp", "message", "log.message"]
    method => "SHA1"
    target => "[@metadata][fingerprint]"
  }
}

output {
  # Route logs to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    user => "${LS_ES_USERNAME:elastic}"
    password => "${LS_ES_PASSWORD:changeme}"
    
    # Index name pattern based on service and date
    index => "thiqax-%{[event][dataset]:.logs}-%{+YYYY.MM.dd}"
    
    # Use event fingerprint as document ID to avoid duplicates
    document_id => "%{[@metadata][fingerprint]}"
    
    # Connection pool settings for better performance
    pool_max => 20
    pool_max_per_route => 10
    timeout => 120
    
    # Retry settings
    retry_on_conflict => 5
    action => "index"
    
    # Template settings
    template_name => "thiqax-logs"
    template_overwrite => true
    
    # ILM Policy 
    ilm_enabled => true
    ilm_rollover_alias => "thiqax"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "thiqax-logs-policy"
  }
  
  # Send critical errors to a separate index for immediate alerting
  if [log][level] == "error" or [http][response][category] == "server_error" {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      user => "${LS_ES_USERNAME:elastic}"
      password => "${LS_ES_PASSWORD:changeme}"
      index => "thiqax-errors-%{+YYYY.MM.dd}"
      document_id => "%{[@metadata][fingerprint]}"
    }
  }
  
  # Output to stdout for debugging in development
  if "${LS_ENVIRONMENT:staging}" == "development" {
    stdout {
      codec => rubydebug
    }
  }
  
  # Send metrics to Prometheus
  if [event][duration] {
    prometheus {
      metric_id => "thiqax_response_time"
      metrics => [
        {
          name => "thiqax_response_time_seconds"
          type => "gauge"
          value => "%{[event][duration]}"
          description => "Response time in seconds"
          labels => {
            "service" => "%{[service]}"
            "endpoint" => "%{[api][endpoint]}"
            "status_code" => "%{[http][response][status_code]}"
          }
        }
      ]
    }
  }
}
